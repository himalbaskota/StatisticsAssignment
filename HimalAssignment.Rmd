---
output:
  html_document: default
  pdf_document: default
---
```
title:"Statistics Assignment"
author: "Himal Baskota"
date: "2025/05/11" 
```
```{r}
Sys.setenv("R_INTERACTIVE_DEVICE" = "png")
```

```{r}
options(repos = c(CRAN = "https://cran.r-project.org"))

if (!require("rsample")) install.packages("rsample")
if (!require("ggplot2")) install.packages("ggplot2")
```

#importing needed library

```{r}
library(rsample)
library(ggplot2)      # For creating plots
```

#importing input data from csv file

```{r}
X=as.matrix(read.csv(file="/Volumes/Mac\'s\ Drive\ /Himal/Softwarica/stat/r-solution/x.csv",header = F))
colnames(X)<-c("x1","x3","x4","x5")
```

#displaying imported data

```{r}
head(X)
```

#importing Output data from y.csv

```{r}
Y=as.matrix(read.csv(file="/Volumes/Mac\'s\ Drive\ /Himal/Softwarica/stat/r-solution/y.csv",header = F))
colnames(Y)<-c("y")
```

#displaying output data

```{r}
head(Y)
```

#importing time series data

```{r}
time = read.csv("/Volumes/Mac\'s\ Drive\ /Himal/Softwarica/stat/r-solution/t.csv", header = F)
time = as.matrix(time)
```

#displaying time series data

```{r}
head(time)
```

#task 1.1 #plotting time series data

```{r}
X.ts<-ts(X,start = c(min(time),max(time)),frequency =1)
Y.ts<-ts(Y,start = c(min(time),max(time)),frequency =1)
```

#plotting timeseries data of input and target variable

```{r}

plot(X.ts[0:50,1],main = "Time series plot of Ambient Temperature", xlab = "Time", ylab = "°C" , col = "#1130ee", type = "l")
plot(X.ts[0:50,2],main = "Time series plot of Atmospheric Pressure", xlab = "Time", ylab = "millibar" , col = "#1130ee", type = "l")
plot(X.ts[0:50,3],main = "Time series plot of Humidity Level", xlab = "Time", ylab = "%" , col = "#1130ee", type = "l")
plot(X.ts[0:50,4],main = "Time series plot of Exhaust Vaccum", xlab = "Time", ylab = "cm Hg" , col = "#1130ee", type = "l")
plot(Y.ts[400:450],main = "Time series plot of Energy Output", xlab = "Time", ylab = "MegaWatt" , col = "#1130ee", type = "l")

```

# task 1.2 : Plotting distribution of each input EEG signal

#Creating a density of all input signal X

```{r}
density_x1=density(X[,"x1"])
density_x3 = density(X[,"x3"])
density_x4 = density(X[,"x4"])
density_x5 = density(X[,"x5"])

# Find the maximum y-value across all densities to set y-axis limit
max_y = max(
  max(density_x1$y),
  max(density_x3$y),
  max(density_x4$y),
  max(density_x5$y)
)
# Set up the plot area with extra space for the legend
par(mar = c(5, 10, 4, 8))  # c(bottom, left, top, right)

plot(density_x1, 
     main = "Density Plots of Input Signals",
     xlab = "Values", 
     ylab = "Density",
     col = "#1130ee",  # Red color for temperature
     ylim = c(0, max_y),
     xlim = c(0,1200),
     lwd = 2)

lines(density_x3, col = "#47b86d", lwd = 2)  # Teal for pressure
lines(density_x4, col = "#996668", lwd = 2)  # Brown for humidity
lines(density_x5, col = "#cedd22", lwd = 2)  # Slateblue for vacuum
par(xpd = TRUE)  # Allow plotting outside the figure region
# Add a legend
legend("topright", 
       legend = c("Temperature (x1)", "Pressure (x3)", "Humidity (x4)", "Vacuum (x5)"),
       col = c("#1130ee", "#47b86d", "#c33c63", "#cedd22"),
       lwd = 2)
par(xpd = FALSE)  # Reset to default

```

# Creating a Histogram of X signal

```{r}
hist(X,freq = FALSE,main = "Density plot of Input Signals",xlab = "Distribution", col = "#47b86d")
```

#combining Histogram of X signal with density plot

```{r}
density_of_X = density(X)
hist(X,freq = FALSE,main = "Density plot with Density line plot of Input signals", xlab = "Distribution", col = "#47b86d")
lines(density_of_X,lwd=2,col="#1130ee")
rug(jitter(X))
```

#histogram and density plot of individual input signal X and output signal y

```{r}
#Creating a density plot of input signal X1 
density_of_X1=density(X[,"x1"])
hist(X[,"x1"],freq = FALSE,main = "Histogram and density plot of Temperature",xlab = "°C", col = "#47b86d")
lines(density_of_X1,lwd=2,col="#1130ee")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x1"]))


#Creating a density plot of input signal X3 
density_of_X3=density(X[,"x3"])
hist(X[,"x3"],freq = FALSE,main = "Histogram and density plot of Pressure",xlab = "millibar", col = "#47b86d")
lines(density_of_X3,lwd=2,col="#1130ee")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x3"]))


#Creating a density plot of input signal X4
density_of_X4=density(X[,"x4"])
hist(X[,"x4"],freq = FALSE,main = "Histogram and density plot of Humidity",xlab = "%", col = "#47b86d")
lines(density_of_X4,lwd=2,col="#1130ee")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x4"]))

#Creating a density plot of input signal X5
density_of_X5=density(X[,"x5"])
hist(X[,"x5"],freq = FALSE,main = "Histogram and density plot of Vacuum",xlab = "cm Hg", col = "#47b86d")
lines(density_of_X5,lwd=2,col="#1130ee")
# Add the data-points with noise in the X-axis
rug(jitter(X[,"x5"]))


#Creating a density plot of output signal y
density_of_y=density(Y[,"y"])
hist(Y[,"y"],freq = FALSE,main = "Histogram and density plot of Energy Output",xlab = "MegaWatt", col = "#47b86d")
lines(density_of_y,lwd=2,col="#1130ee")
# Add the data-points with noise in the X-axis
rug(jitter(Y[,"y"]))

```

# Task 1.3 creating scatter plots to indeitify correlation

# arrenging plot in a single screen

```{r}
par(mfrow=c(2,2))

# Plotting input signal X1 against output signal Y
plot(X[,"x1"],Y,main = "Correlation betweeen X1 and Y signal", xlab = "X1 signal", ylab = "Output signal y", col = "#47b86d")

# Plotting input signal X3 against output signal Y
plot(X[,"x3"],Y,main = "Correlation betweeen X3 and Y signal", xlab = "X3 signal", ylab = "Output signal y", col = "#47b86d")

# Plotting input signal X4 against output signal Y
plot(X[,"x4"],Y,main = "Correlation betweeen X4 and Y signal", xlab = "X4 signal", ylab = "Output signal y", col = "#47b86d")

# Plotting input signal X4 against output signal Y
plot(X[,"x5"],Y,main = "Correlation betweeen X5 and Y signal", xlab = "X5 signal", ylab = "Output signal y", col = "#47b86d")

```

```{r}
# Q-Q plots to check for normality
qqnorm(X[,"x1"], col = "#47b86d",main = "QQ plot of Temperature")
qqline(X[,"x1"], col = "#1130ee")

qqnorm(X[,"x3"], col = "#47b86d",main = "QQ plot of Pressure")
qqline(X[,"x3"], col = "#1130ee")

qqnorm(X[,"x4"], col = "#47b86d",main = "QQ plot of Humidity")
qqline(X[,"x4"], col = "#1130ee")

qqnorm(X[,"x5"], col = "#47b86d",main = "QQ plot of Vacuum")
qqline(X[,"x5"], col = "#1130ee")

```

```{r}

```



# Task 2

# Calculating ones for binding the data

```{r}

ones = matrix(1 , length(X)/4,1)
head(ones)
```

# Task 2.1

# Calculating thetahat of each candidate model

```{r}
#Binding data from equation of model 1.
# Model 1 predictors

X_model1_raw <- cbind(X[,"x4"], X[,"x3"]^2)

# Scale predictors (not the intercept)
X_model1_scaled <- scale(X_model1_raw)

# Add intercept back
X_model1 <- cbind(ones, X_model1_scaled)

head(X_model1)

#Calculating thetahat of Model 1
Model1_thetahat=solve(t(X_model1) %*% X_model1) %*% t(X_model1) %*% Y
Model1_thetahat


#For model 2
#Binding data from equation of model 2.
# Model 2 predictors
X_model2_raw <- cbind(X[,"x4"],X[,"x3"]^2,X[,"x5"])

# Scale predictors (not the intercept)
X_model2_scaled <- scale(X_model2_raw)

# Add intercept back
X_model2 <- cbind(ones, X_model2_scaled)

head(X_model2)

#Calculating thetahat of Model 2
Model2_thetahat=solve(t(X_model2) %*% X_model2) %*% t(X_model2) %*% Y
Model2_thetahat

#Model 3

#Binding data from equation of model 3.

# Model 3 predictors
X_model3_raw <- cbind(X[,"x3"],X[,"x4"],X[,"x5"]^3)

# Scale predictors (not the intercept)
X_model3_scaled <- scale(X_model3_raw)

# Add intercept back
X_model3 <- cbind(ones, X_model3_scaled)

head(X_model3)

#Calculating thetahat of Model 3
Model3_thetahat=solve(t(X_model3) %*% X_model3) %*% t(X_model3) %*% Y
Model3_thetahat



#For model 4
#Binding data from equation of model 4.

# Model 4 predictors
X_model4_raw <- cbind(X[,"x4"],(X[,"x3"])^2,(X[,"x5"])^3)

# Scale predictors (not the intercept)
X_model4_scaled <- scale(X_model4_raw)

# Add intercept back
X_model4 <- cbind(ones, X_model4_scaled)

head(X_model4)

#Calculating thetahat of Model 4
Model4_thetahat=solve(t(X_model4) %*% X_model4) %*% t(X_model4) %*% Y
Model4_thetahat


# for Model 5
#Binding data from equation of model 5.

# Model 5 predictors
X_model5_raw <- cbind((X[,"x4"]),(X[,"x1"])^2,(X[,"x3"])^2)

# Scale predictors (not the intercept)
X_model5_scaled <- scale(X_model5_raw)

# Add intercept back
X_model5 <- cbind(ones, X_model5_scaled)

head(X_model5)

#Calculating thetahat of Model 5
Model5_thetahat=solve(t(X_model5) %*% X_model5) %*% t(X_model5) %*% Y
Model5_thetahat

```

# printing value of theta of each model

```{r}
#model1
Model1_thetahat
t(Model1_thetahat)
#model 2
Model2_thetahat
t(Model2_thetahat)
#model 3
Model3_thetahat
t(Model3_thetahat)
#model 4
Model4_thetahat
t(Model4_thetahat)
#model 5
Model5_thetahat
t(Model5_thetahat)
```

# Task 2.2

#Calculating Y-hat and RSS for each model

```{r}
#Calculating Y-hat and RSS Model 1
Y_hat_model1 = X_model1 %*% Model1_thetahat
head(Y_hat_model1)
#Calculating RSS
RSS_Model_1=sum((Y-Y_hat_model1)^2)
head(RSS_Model_1)

# Calculating Y-hat and RSS of model 2
Y_hat_model2 = X_model2 %*% Model2_thetahat
head(Y_hat_model2)
#Calculating RSS
RSS_Model_2=sum((Y-Y_hat_model2)^2)
head(RSS_Model_2)

# Calculating Y-hat and RSS of model 3
Y_hat_model3 = X_model3 %*% Model3_thetahat
head(Y_hat_model3)
#Calculating RSS
RSS_Model_3=sum((Y-Y_hat_model3)^2)
head(RSS_Model_3)
 
# Calculating Y-hat and RSS of model 4
Y_hat_model4 = X_model4 %*% Model4_thetahat
head(Y_hat_model4)
#Calculating RSS
RSS_Model_4=sum((Y-Y_hat_model4)^2)
head(RSS_Model_4)

# Calculating Y-hat and RSS of model 5
Y_hat_model5 = X_model5 %*% Model5_thetahat
head(Y_hat_model5)
#Calculating RSS
RSS_Model_5=sum((Y-Y_hat_model5)^2)
head(RSS_Model_5)
```

#printing RSS value

```{r}
model1 <- c(RSS_Model_1)
model2 <- c(RSS_Model_2)
model3 <- c(RSS_Model_3)
model4 <- c(RSS_Model_4)
model5 <- c(RSS_Model_5)

dfRSS <- data.frame(model1, model2,model3,model4,model5)
dfRSS

```

#Task 2.3 Calculating likelihood and Variance of each model

```{r}
N=length(Y)

#Calculating the Variance of Model 1
Variance_model1=RSS_Model_1/(N-1)
Variance_model1

#Calculating the log-likelihood of Model 1
likehood_Model_1=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model1))-(1/(2*Variance_model1))*RSS_Model_1
likehood_Model_1

#Calculating Variance and log-likelihood of Model 2
Variance_model2=RSS_Model_2/(N-1)
Variance_model2
likehood_Model_2=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model2))-(1/(2*Variance_model2))*RSS_Model_2
likehood_Model_2


#Calculating Variance and log-likelihood of Model 3
Variance_model3=RSS_Model_3/(N-1)
Variance_model3
likehood_Model_3=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model3))-(1/(2*Variance_model3))*RSS_Model_3
likehood_Model_3

#Calculating Variance and log-likelihood of Model 4
Variance_model4=RSS_Model_4/(N-1)
Variance_model4
likehood_Model_4=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model4))-(1/(2*Variance_model4))*RSS_Model_4
likehood_Model_4

#Calculating Variance and log-likelihood of Model 5
Variance_model5=RSS_Model_5/(N-1)
Variance_model5
likehood_Model_5=
  -(N/2)*(log(2*pi))-(N/2)*(log(Variance_model5))-(1/(2*Variance_model5))*RSS_Model_5
likehood_Model_5
```

#printing variance values

```{r}
model1 <- c(Variance_model1)
model2 <- c(Variance_model2)
model3 <- c(Variance_model3)
model4 <- c(Variance_model4)
model5 <- c(Variance_model5)

dfVariance <- data.frame(model1, model2,model3,model4,model5)
dfVariance
```

#printing likelihood values

```{r}
model1 <- c(likehood_Model_1)
model2 <- c(likehood_Model_2)
model3 <- c(likehood_Model_3)
model4 <- c(likehood_Model_4)
model5 <- c(likehood_Model_5)

dfLikelihood <- data.frame(model1, model2,model3,model4,model5)
dfLikelihood
```

# Task 2.4

# Calculating AIC And BIC of each model

```{r}
# Calculating AIC and BIC of model 1
K_model1<-length(Model1_thetahat)
K_model1
AIC_model1=2*K_model1-2*likehood_Model_1
AIC_model1
BIC_model1=K_model1*log(N)-2*likehood_Model_1
BIC_model1

## thetahat of model 2
K_model2<-length(Model2_thetahat)
K_model2
##Calculating AIC and BIC of model 2
AIC_model2=2*K_model2-2*likehood_Model_2
AIC_model2
BIC_model2=K_model2*log(N)-2*likehood_Model_2
BIC_model2

## thetahat of model 3
K_model3<-length(Model3_thetahat)
K_model3
##Calculating AIC and BIC of model 3
AIC_model3=2*K_model3-2*likehood_Model_3
AIC_model3
BIC_model3=K_model3*log(N)-2*likehood_Model_3
BIC_model3

## thetahat of model 4
K_model4<-length(Model4_thetahat)
K_model4
##Calculating AIC and BIC of model 4
AIC_model4=2*K_model4-2*likehood_Model_4
AIC_model4
BIC_model4=K_model4*log(N)-2*likehood_Model_4
BIC_model4

## thetahat of model 5
K_model5<-length(Model5_thetahat)
K_model5
##Calculating AIC and BIC of model 5
AIC_model5=2*K_model5-2*likehood_Model_5
AIC_model5
BIC_model5=K_model5*log(N)-2*likehood_Model_5
BIC_model5
```

#printing K values

```{r}
model1 <- c(K_model1)
model2 <- c(K_model2)
model3 <- c(K_model3)
model4 <- c(K_model4)
model5 <- c(K_model5)

dfK <- data.frame(model1, model2,model3,model4,model5)
dfK
```

#printing AIC values

```{r}
model1 <- c(AIC_model1)
model2 <- c(AIC_model2)
model3 <- c(AIC_model3)
model4 <- c(AIC_model4)
model5 <- c(AIC_model5)

dfAIC <- data.frame(model1, model2,model3,model4,model5)
dfAIC
```

#printing BIC values

```{r}
model1 <- c(BIC_model1)
model2 <- c(BIC_model2)
model3 <- c(BIC_model3)
model4 <- c(BIC_model4)
model5 <- c(BIC_model5)

dfBIC <- data.frame(model1, model2,model3,model4,model5)
dfBIC
```

## Task 2.5 calculating error plotting normal/gaussian distibution of each plot

```{r}
par(mfrow=c(1,1))

## Error of model1
model1_error <- Y-Y_hat_model1
head(model1_error)

## Plotting the graph QQplot and QQ line of model 1
qqnorm(model1_error, col = "#47b86d",main = "QQ plot of Model 1 (y = θ1x4 + θ2x3² + θbias)")
qqline(model1_error, col = "#1130ee",lwd=1)


## Error of model2
model2_error <- Y-Y_hat_model2 # error of model 2
## Plotting QQplot and QQ line of model 2
qqnorm(model2_error, col = "#47b86d",main = "QQ plot of Model 2 (y = θ1x4 + θ2x3² + θ3x5 + θbias)")
qqline(model2_error, col = "#1130ee")


## Error of model3
model3_error <- Y- Y_hat_model3
## Plotting QQplot and QQ line of model 3
qqnorm(model3_error, col = "#47b86d",main = "QQ plot of Model 3 (y = θ1x3 + θ2x4 + θ3x5 + θbias)")
qqline(model3_error, col = "#1130ee")

## Error of model4
model4_error <- Y-Y_hat_model4
## Plotting QQplot and QQ line of model 4
qqnorm(model4_error, col = "#47b86d",main = "QQ plot of Model 4 (y = θ1x4 + θ2x3² + θ3x5³ + θbias)")
qqline(model4_error, col = "#1130ee")

## Error of model5
model5_error <- Y- Y_hat_model5
## Plotting QQplot and QQ line of model 5
qqnorm(model5_error, col = "#47b86d",main = "QQ plot of Model 5 (y = θ1x4 + θ2x1² + θ3x3³ + θbias)")
qqline(model5_error, col = "#1130ee")
```

# Task 2.7 splitting data into training and testing dataset and calculating estamation based on training dataset

#also plotting normal distribution graph of training data
```{r}
 # Function to calculate Z-scores
calculate_z_scores <- function(data) {
  if(is.matrix(data) || is.data.frame(data)) {
    return(apply(data, 2, function(x) abs((x - mean(x)) / sd(x))))
  } else {
    return(abs((data - mean(data)) / sd(data)))
  }
}

# Calculate Z-scores for input variables X
z_scores_X <- calculate_z_scores(X)

# Identify rows where any X variable has a Z-score > 3.0
outlier_rows_X <- which(apply(z_scores_X, 1, function(row) any(row > 3.0)))

# Print summary of outliers in predictors only
cat("Number of outliers in input variables:", length(outlier_rows_X), "\n")
cat("Percentage of data with outliers:", round(100 * length(outlier_rows_X) / nrow(X), 2), "%\n")

# Remove outliers from both X and Y (maintaining correspondence)
X_clean <- X[-outlier_rows_X, ]
Y_clean <- Y[-outlier_rows_X, ]

# Verify dimensions match
cat("\nDimensions after outlier removal:\n")
cat("X dimensions:", dim(X_clean), "\n")
cat("Y dimensions:", length(Y_clean), "\n")
cat("Remaining data points:", nrow(X_clean), "\n")

# Optional: Print statistical summary of cleaned data
cat("\nSummary of cleaned input variables:\n")
print(colMeans(X_clean))
cat("\nSummary of cleaned target variable:\n")
cat("Mean:", mean(Y_clean), "\n")
cat("SD:", sd(Y_clean), "\n")
```


```{r}
## Spliting the dataset y into  Traning and testing data set.
split_Y<-initial_split(data = as.data.frame(Y),prop=.7)
## Traning splitted Y dataset 
Y_training_set<-training(split_Y)
Y_testing_set<-as.matrix(testing(split_Y))
## Testing splitted Y dataset 
Y_training_data<-as.matrix(Y_training_set)

## Spliting the dataset of X into  Traning and testing data set.
split_X<-initial_split(data = as.data.frame(X),prop=.7)
## Traning splitted X dataset
X_training_set<-training(split_X)
## Testing splitted X dataset 
X_testing_set<-as.matrix(testing(split_X))
X_testing_data<-as.matrix(X_testing_set)
X_training_data<-as.matrix(X_training_set)

# Create raw training predictors for model 5
X_train_model_raw <- cbind(
  x4 = X_training_set[,"x4"],
  x1_sq = X_training_set[,"x1"]^2,
  x3_sq = X_training_set[,"x3"]^2
)

# Apply scaling
X_train_scaled <- scale(X_train_model_raw)

# Save scaling parameters
center_vals <- attr(X_train_scaled, "scaled:center")
scale_vals  <- attr(X_train_scaled, "scaled:scale")

# Create final training matrix with intercept
training_ones <- matrix(1, nrow(X_train_scaled), 1)
X_traning_model <- cbind(training_ones, X_train_scaled)

# Estimate thetahat
traning_thetahat <- solve(t(X_traning_model) %*% X_traning_model) %*% 
                    t(X_traning_model) %*% Y_training_data

# === Apply same scaling to test set === #
X_test_model_raw <- cbind(
  x4 = X_testing_set[,"x4"],
  x1_sq = X_testing_set[,"x1"]^2,
  x3_sq = X_testing_set[,"x3"]^2
)

# Apply the same centering and scaling from training set
X_test_scaled <- scale(X_test_model_raw, center = center_vals, scale = scale_vals)

# Final test matrix
test_ones <- matrix(1, nrow(X_test_scaled), 1)
X_test_model <- cbind(test_ones, X_test_scaled)

### Estimating model parameters using Traning set
#traning_ones=matrix(1 , length(X_training_set$x1),1)
# selected model 2 and using equation of model 5
#X_traning_model<-cbind(traning_ones,X_training_set[,"x4"],(X_training_set[,"x1"])^2,(X_training_set[,"x3"])^2)

traning_thetahat=solve(t(X_traning_model) %*% X_traning_model) %*% t(X_traning_model) %*%  Y_training_data
  
### Model out/Prediction
Y_testing_hat = X_test_model  %*% traning_thetahat
head(Y_testing_hat)
RSS_testing <- sum((Y_testing_set-Y_testing_hat)^2)
head(RSS_testing)
t.test(X_traning_model, mu=500, alternative="two.sided", conf.level=0.95)
C_I1=-0.2783950
C_I2=0.2762101
p2 <- plot(density(X_traning_model), col="#1130ee", lwd=2,
          main="Distribution of Traning Data")
abline(v=C_I1,col="#996668", lty=2)
abline(v=C_I2,col="#996668", lty=2)

thetaHat_training =solve(t(X_training_data) %*% X_training_data) %*% t(X_training_data) %*%Y_training_data
thetaHat_training
length(thetaHat_training)
dis_test=density(Y_training_data)
plot((dis_test), main = "Density plot of Output Training Data", col = "#1130ee")


```

```{r}
# === Proper 95% Confidence Interval Calculation === #
# 1. Calculate residuals and estimate error variance from training data
Y_training_hat <- X_traning_model %*% traning_thetahat
residuals <- Y_training_data - Y_training_hat
RSS_train <- sum(residuals^2)
n_train <- nrow(X_traning_model)
p <- ncol(X_traning_model)  # Number of parameters (including intercept)
sigma_squared <- RSS_train / (n_train - p)  # Residual variance estimate

# 2. Calculate parameter covariance matrix
param_cov_matrix <- sigma_squared * solve(t(X_traning_model) %*% X_traning_model)

# 3. Calculate prediction intervals for each test point
n_test <- nrow(X_test_model)
lower_CI <- numeric(n_test)
upper_CI <- numeric(n_test)
t_crit <- qt(0.975, df = n_train - p)  # Critical t-value for 95% CI

for (i in 1:n_test) {
  x_i <- X_test_model[i,]
  
  # Prediction variance has two components:
  # 1. Variance due to parameter uncertainty: x_i' * (X'X)^(-1) * x_i * sigma^2
  # 2. Variance due to random error: sigma^2
  pred_var_params <- t(x_i) %*% param_cov_matrix %*% x_i
  pred_variance <- sigma_squared + as.numeric(pred_var_params)
  
  # Calculate margin of error
  margin <- t_crit * sqrt(pred_variance)
  
  # Store CI bounds
  lower_CI[i] <- Y_testing_hat[i] - margin
  upper_CI[i] <- Y_testing_hat[i] + margin
}

# === Create Required Visualization === #
# Create data frame for plotting
plot_data <- data.frame(
  Index = 1:n_test,
  Actual = Y_testing_set,
  Predicted = Y_testing_hat,
  Lower_CI = lower_CI,
  Upper_CI = upper_CI
)

# Calculate performance metrics
MSE <- mean((Y_testing_set - Y_testing_hat)^2)
RMSE <- sqrt(MSE)
R_squared <- 1 - sum((Y_testing_set - Y_testing_hat)^2) / sum((Y_testing_set - mean(Y_testing_set))^2)

cat("Model 5 Performance Metrics:\n")
cat("MSE:", MSE, "\n")
cat("RMSE:", RMSE, "\n")
cat("R-squared:", R_squared, "\n")

# Create plot using base R
par(mar = c(5, 5, 4, 2))  # Increase left margin for y-axis label

# Sort data points by actual values for clearer visualization
sorted_indices <- order(Y_testing_set)

# Plot actual vs. sample index
plot(plot_data$Index[sorted_indices], 
     Y_testing_set[sorted_indices],
     type = "p", 
     pch = 16,
     col = "#47b86d",
     xlab = "Test Sample Index", 
     ylab = "Energy Output (MW)",
     main = "Model 5: Predictions with 95% Confidence Intervals",
     ylim = range(c(lower_CI, upper_CI, Y_testing_set)))

# Add prediction line
lines(plot_data$Index[sorted_indices], 
      Y_testing_hat[sorted_indices], 
      col = "#1130ee", 
      lwd = 2)

# Add confidence interval as a shaded area
polygon(c(plot_data$Index[sorted_indices], rev(plot_data$Index[sorted_indices])),
        c(lower_CI[sorted_indices], rev(upper_CI[sorted_indices])),
        col = rgb(0.7, 0.7, 0.7, 0.3), 
        border = NA)

# Add error bars for select points (to avoid overcrowding)
for (i in seq(1, length(sorted_indices), 10)) {
  idx <- sorted_indices[i]
  segments(plot_data$Index[idx], lower_CI[idx], 
           plot_data$Index[idx], upper_CI[idx], 
           col = "gray40")
}

# Add legend
legend("topleft",
       legend = c("Actual Values", "Predicted Values", "95% Confidence Interval"),
       col = c("#47b86d", "#1130ee", "gray70"),
       pch = c(16, NA, 15),
       lty = c(NA, 1, NA),
       pt.cex = c(1, 0, 2),
       bg = "white")

# Add result text
mtext(paste("RMSE =", round(RMSE, 2), 
            "   R² =", round(R_squared, 3)), 
      side = 3, 
      line = 0.5, 
      cex = 0.8)
```


#Task 3

```{r}
## Model 5 will be used, parameter are selected and kept constant.
arr_1=0
arr_2=0
f_value=0
s_value=0
Model5_thetahat
#values from thetahat
thetebias <- 454.365009 #selected parameter
thetaone <- 1.349107 # selected parameter
thetatwo <- -10.605331 # constant value
thetathree <- -5.173124 # constant value


Epison <- RSS_Model_5 * 2 ## fixing value of eplision
num <- 100 #number of iteration
##Calculating Y-hat for performing rejection ABC
counter <- 0
for (i in 1:num) {
  range1 <- runif(1, -454.365009, 454.365009) # calculating the range
  range1
  range2 <- runif(1, -1.349107, 1.349107)
  New_thetahat <- matrix(c(range1,range2,thetatwo,thetathree))
  New_Y_Hat <- X_model5 %*% New_thetahat ## calculating new Y-hat
  new_RSS <- sum((Y-New_Y_Hat)^2)
  new_RSS
  if (new_RSS > Epison){
    arr_1[i] <- range1
    arr_2[i] <- range2
    counter = counter+1
    f_value <- matrix(arr_1)
    s_value <- matrix(arr_2)
  }
}
hist(f_value , col = "#47b86d")
hist(s_value , col = "#47b86d")

###ploting Joint and Marginal Posterior Distribution of the graph
plot(f_value,s_value, col = c("#47b86d", "#1130ee"), main = "Joint and Marginal Posterior Distribution (Rejection Range)")
par(mfrow=c(1,1))
```
